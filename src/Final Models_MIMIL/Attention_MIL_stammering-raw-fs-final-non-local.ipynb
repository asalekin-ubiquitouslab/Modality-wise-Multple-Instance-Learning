{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "seed = 11\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X1,Y):\n",
    "        self.X1 = X1\n",
    "        self.Y = Y\n",
    "    def __len__(self):        \n",
    "        return len(self.X1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X1[index]\n",
    "        y = self.Y[index]\n",
    "        return x,y\n",
    "\n",
    "#from model import Attention, GatedAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negetive pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1121, 1, 19, 24) (1121,)\n",
      "(190, 1, 19, 24) (190,)\n",
      "(199, 1, 19, 24) (199,)\n"
     ]
    }
   ],
   "source": [
    "xTrain=np.load(\"../extracted_features/FS/input_features_free_train.npy\")\n",
    "\n",
    "yTrain=np.load(\"../extracted_features/FS/label_free_train.npy\")\n",
    "\n",
    "xTest=np.load(\"../extracted_features/FS/input_features_free_test.npy\")\n",
    "\n",
    "yTest=np.load(\"../extracted_features/FS/label_free_test.npy\")\n",
    "\n",
    "xVal=np.load(\"../extracted_features/FS/input_features_free_val.npy\")\n",
    "\n",
    "yVal=np.load(\"../extracted_features/FS/label_free_val.npy\")\n",
    "\n",
    "xTrain = xTrain.reshape(len(xTrain),1,19,24)\n",
    "xTest = xTest.reshape(len(xTest),1,19,24)\n",
    "xVal = xVal.reshape(len(xVal),1,19,24)\n",
    "\n",
    "print(xTrain.shape,yTrain.shape)\n",
    "print(xTest.shape,yTest.shape)\n",
    "print(xVal.shape,yVal.shape)\n",
    "\n",
    "\n",
    "traindataset = Dataset(xTrain,yTrain)\n",
    "testdataset = Dataset(xTest,yTest)\n",
    "valdataset = Dataset(xVal,yVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stammering(data_utils.Dataset):\n",
    "    def __init__(self, target_number=1, mean_bag_length=5, var_bag_length=2, num_bag=150, seed=2021, train=\"train\"):\n",
    "        self.target_number = target_number\n",
    "        self.mean_bag_length = mean_bag_length\n",
    "        self.var_bag_length = var_bag_length\n",
    "        self.num_bag = num_bag\n",
    "        self.train = train\n",
    "        self.r = np.random.RandomState(seed)\n",
    "\n",
    "        if self.train==\"train\":\n",
    "            self.train_bags_list, self.train_labels_list = self._create_bags()\n",
    "        elif self.train==\"val\":\n",
    "            self.val_bags_list, self.val_labels_list = self._create_bags()\n",
    "        else:\n",
    "            self.test_bags_list, self.test_labels_list = self._create_bags()\n",
    "\n",
    "    def _create_bags(self):\n",
    "        if self.train==\"train\":\n",
    "            print(\"train\")\n",
    "            loader = data_utils.DataLoader(traindataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "        elif self.train==\"val\":\n",
    "            print(\"val\")\n",
    "            loader = data_utils.DataLoader(valdataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "        else:\n",
    "            loader = data_utils.DataLoader(testdataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "            \n",
    "        bags_list = []\n",
    "        labels_list = []\n",
    "        for (batch_data, batch_labels) in loader:\n",
    "            #print(batch_data.shape)\n",
    "            bags_list.append(batch_data.reshape(19,1,24))\n",
    "            temp = torch.as_tensor(np.array([batch_labels for x in range(19)]))\n",
    "            labels_list.append(temp)\n",
    "            \n",
    "               \n",
    "        #print(bags_list)\n",
    "        #print(labels_list)\n",
    "        return bags_list, labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train==\"train\":\n",
    "            return len(self.train_labels_list)\n",
    "        elif self.train==\"val\":\n",
    "            return len(self.val_labels_list)\n",
    "        else:\n",
    "            return len(self.test_labels_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train==\"train\":\n",
    "            bag = self.train_bags_list[index]\n",
    "            label = [max(self.train_labels_list[index]), self.train_labels_list[index]]\n",
    "        elif self.train==\"val\":\n",
    "            bag = self.val_bags_list[index]\n",
    "            label = [max(self.val_labels_list[index]), self.val_labels_list[index]]\n",
    "        else:\n",
    "            bag = self.test_bags_list[index]\n",
    "            label = [max(self.test_labels_list[index]), self.test_labels_list[index]]\n",
    "\n",
    "        return bag, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "train_loader = data_utils.DataLoader(Stammering(train=\"train\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)\n",
    "test_loader = data_utils.DataLoader(Stammering(train=\"val\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)\n",
    "val_loader = data_utils.DataLoader(Stammering(train=\"test\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _NonLocalBlockND(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n",
    "        super(_NonLocalBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [1, 2, 3]\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "            bn = nn.BatchNorm3d\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "            bn = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_nd = nn.Conv1d\n",
    "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
    "            bn = nn.BatchNorm1d\n",
    "\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                        kernel_size=1, stride=1, padding=0),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant(self.W[1].weight, 0)\n",
    "            nn.init.constant(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant(self.W.weight, 0)\n",
    "            nn.init.constant(self.W.bias, 0)\n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class NONLocalBlock1D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock1D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=1, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)\n",
    "class NONLocalBlock2D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock2D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=2, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)\n",
    "# class NonLocalBlock(nn.Module):\n",
    "#     def __init__(self, channel):\n",
    "#         super(NonLocalBlock, self).__init__()\n",
    "#         self.inter_channel = channel\n",
    "#         self.conv_phi = nn.Conv1d(in_channels=channel, out_channels=self.inter_channel, kernel_size=1, stride=1,padding=0, bias=False)\n",
    "#         self.conv_theta = nn.Conv1d(in_channels=channel, out_channels=self.inter_channel, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "#         self.conv_g = nn.Conv1d(in_channels=channel, out_channels=self.inter_channel, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "#         self.conv_mask = nn.Conv1d(in_channels=self.inter_channel, out_channels=channel, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # [N, C, H , W]\n",
    "#         b, c, w = x.size()\n",
    "#         # [N, C/2, H * W]\n",
    "# #         print(b, c, h, w)\n",
    "# #         print(self.conv_phi(x).view(b, c, -1).shape)\n",
    "#         x_phi = self.conv_phi(x).view(b,c, -1)\n",
    "#         # [N, H * W, C/2]\n",
    "#         x_theta = self.conv_theta(x).view(b, -1,c).contiguous()\n",
    "#         x_g = self.conv_g(x).view(b, -1,c).contiguous()\n",
    "# #         print(x_theta.shape)\n",
    "# #         print(x_phi.shape)\n",
    "# #         print(x_g.shape)\n",
    "#         # [N, H * W, H * W]\n",
    "#         mul_theta_phi = torch.matmul(x_theta, x_phi)\n",
    "#         mul_theta_phi = self.softmax(mul_theta_phi)\n",
    "#         # [N, H * W, C/2]\n",
    "#         mul_theta_phi_g = torch.matmul(mul_theta_phi, x_g)\n",
    "#         # [N, C/2, H, W]\n",
    "#         mul_theta_phi_g = mul_theta_phi_g.permute(0,2,1).contiguous().view(b,self.inter_channel, w)\n",
    "#         # [N, C, H , W]\n",
    "#         mask = self.conv_mask(mul_theta_phi_g)\n",
    "#         out = mask + x\n",
    "#         return out\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.L = 128\n",
    "        self.D = 256\n",
    "        self.K = 1\n",
    "        \n",
    "        self.nl = NONLocalBlock1D(4)\n",
    "        \n",
    "#         self.nl=nn.Conv1d(in_channels=4, out_channels=4, kernel_size=1, stride=1,padding=0, bias=False)\n",
    "        \n",
    "        self.lossfn=nn.MSELoss()\n",
    "        \n",
    "        self.attention_transform_M = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.L*self.K*4, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512,self.K)\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_HR = nn.Sequential(\n",
    "#             nn.Linear(6,32),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(6,self.D),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.feature_extractor_EDA = nn.Sequential(\n",
    "#             nn.Linear(6,32),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(6,self.D),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.feature_extractor_RSPR= nn.Sequential(\n",
    "#             nn.Linear(6,32),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(6,self.D),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.feature_extractor_RSPA= nn.Sequential(\n",
    "#             nn.Linear(6,16),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(6,self.D),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.feature_transform_HR = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(self.D, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True),\n",
    "            nn.Linear(self.D, self.L)\n",
    "        )\n",
    "        \n",
    "        self.attention_transform_HR = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.D, self.L),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.L,self.K)\n",
    "        )\n",
    "        \n",
    "        self.feature_transform_EDA = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(self.D, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True),\n",
    "            nn.Linear(self.D, self.L)\n",
    "        )\n",
    "        \n",
    "        self.attention_transform_EDA = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.D, self.L),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.L,self.K)\n",
    "        )\n",
    "        \n",
    "        self.feature_transform_RSPA = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(self.D, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True),\n",
    "            nn.Linear(self.D, self.L)\n",
    "        )\n",
    "        \n",
    "        self.attention_transform_RSPA = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.D, self.L),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.L,self.K)\n",
    "        )\n",
    "        \n",
    "        self.feature_transform_RSPR = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm1d(self.D, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True),\n",
    "            nn.Linear(self.D, self.L)\n",
    "        )\n",
    "        \n",
    "        self.attention_transform_RSPR = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.D, self.L),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.L,self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K*4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def attention_pooling(self,feature_embed_HR,feature_embed_EDA,feature_embed_RSPA,feature_embed_RSPR):\n",
    "        \n",
    "        #HR\n",
    "        feature_transform_HR = self.feature_transform_HR(feature_embed_HR)\n",
    "        \n",
    "        A_t_hr = self.attention_transform_HR(feature_embed_HR)\n",
    "        \n",
    "        A_t_hr = torch.transpose(A_t_hr, 1, 0)\n",
    "        \n",
    "        A_t_hr = F.softmax(A_t_hr, dim=1)\n",
    "\n",
    "        M_hr = torch.mm(A_t_hr, feature_transform_HR)\n",
    "        \n",
    "        #EDA\n",
    "        \n",
    "        feature_transform_EDA = self.feature_transform_HR(feature_embed_EDA)\n",
    "        \n",
    "        A_t_eda = self.attention_transform_EDA(feature_embed_EDA)\n",
    "        \n",
    "        A_t_eda = torch.transpose(A_t_eda, 1, 0)\n",
    "        \n",
    "        A_t_eda = F.softmax(A_t_eda, dim=1)\n",
    "\n",
    "        M_eda = torch.mm(A_t_eda, feature_transform_EDA)\n",
    "        \n",
    "        #RSPA\n",
    "\n",
    "        feature_transform_RSPA = self.feature_transform_RSPA(feature_embed_RSPA)\n",
    "        \n",
    "        A_t_rspa = self.attention_transform_RSPA(feature_embed_RSPA)\n",
    "        \n",
    "        A_t_rspa = torch.transpose(A_t_rspa, 1, 0)\n",
    "        \n",
    "        A_t_rspa = F.softmax(A_t_rspa, dim=1)\n",
    "\n",
    "        M_rspa = torch.mm(A_t_rspa, feature_transform_RSPA)\n",
    "        \n",
    "        #RSPR\n",
    "\n",
    "        feature_transform_RSPR = self.feature_transform_RSPR(feature_embed_RSPR)\n",
    "        \n",
    "        A_t_rspr = self.attention_transform_RSPR(feature_embed_RSPR)\n",
    "        \n",
    "        A_t_rspr = torch.transpose(A_t_rspr, 1, 0)\n",
    "        \n",
    "        A_t_rspr = F.softmax(A_t_rspr, dim=1)\n",
    "\n",
    "        M_rspr = torch.mm(A_t_rspr, feature_transform_RSPR)\n",
    "        \n",
    "        M=torch.cat((M_hr,M_eda,M_rspa,M_rspr),dim=0)\n",
    "        \n",
    "        return M\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        x=x.squeeze(0)\n",
    "        \n",
    "        #HR\n",
    "        \n",
    "        feature_embed_HR= self.feature_extractor_HR(x[:,:,:6])\n",
    "        \n",
    "\n",
    "        \n",
    "        #EDA\n",
    "        \n",
    "        feature_embed_EDA= self.feature_extractor_EDA(x[:,:,6:12])\n",
    "        \n",
    "        \n",
    "        #RSPA\n",
    "        \n",
    "        feature_embed_RSPA= self.feature_extractor_RSPA(x[:,:,12:18])\n",
    "        \n",
    "        \n",
    "        #RSPR\n",
    "        \n",
    "        feature_embed_RSPR= self.feature_extractor_RSPR(x[:,:,18:24])\n",
    "        \n",
    "        \n",
    "        \n",
    "        M = self.attention_pooling(feature_embed_HR,feature_embed_EDA,feature_embed_RSPA,feature_embed_RSPR)\n",
    "        \n",
    "#         print(M.shape)\n",
    "        \n",
    "        M = M.reshape(1,4,self.L)\n",
    "        \n",
    "#         print(M.shape)\n",
    "        \n",
    "        M = self.nl(M) \n",
    "        \n",
    "#         print(M.shape)\n",
    "        \n",
    "        M=M.reshape(M.size(0),-1)\n",
    "        \n",
    "#         print(M.shape)\n",
    "\n",
    "        Y_prob = self.classifier(M)        \n",
    "        \n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat\n",
    "\n",
    "\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _ = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        loss=self.lossfn(Y_prob.squeeze(1),Y)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "model = Attention()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model.cuda(),(1,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH  0\n",
      "Train : Loss: 0.2441, Train error: 0.4228, Train acc : 0.6353846153846153\n",
      "Val acc :0.5531914893617021\n",
      "Best validation accuracy  0\n",
      "test acc :0.4588235294117647\n",
      "EPOCH  1\n",
      "Train : Loss: 0.2379, Train error: 0.3880, Train acc : 0.679439941046426\n",
      "Val acc :0.5541125541125541\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.5541125541125541\n",
      "test acc :0.5502183406113537\n",
      "EPOCH  2\n",
      "Train : Loss: 0.2307, Train error: 0.3684, Train acc : 0.697878566203365\n",
      "Val acc :0.5421686746987953\n",
      "Best validation accuracy  0.5541125541125541\n",
      "test acc :0.40506329113924056\n",
      "EPOCH  3\n",
      "Train : Loss: 0.2273, Train error: 0.3747, Train acc : 0.6645367412140575\n",
      "Val acc :0.5625\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.34210526315789475\n",
      "EPOCH  4\n",
      "Train : Loss: 0.2198, Train error: 0.3613, Train acc : 0.6848249027237354\n",
      "Val acc :0.5026737967914437\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.4971751412429378\n",
      "EPOCH  5\n",
      "Train : Loss: 0.2087, Train error: 0.3327, Train acc : 0.7032617342879873\n",
      "Val acc :0.4276729559748428\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.4186046511627907\n",
      "EPOCH  6\n",
      "Train : Loss: 0.2062, Train error: 0.3283, Train acc : 0.7065390749601277\n",
      "Val acc :0.48587570621468923\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.45977011494252873\n",
      "EPOCH  7\n",
      "Train : Loss: 0.1997, Train error: 0.3327, Train acc : 0.7092751363990647\n",
      "Val acc :0.4692737430167598\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.4581005586592179\n",
      "EPOCH  8\n",
      "Train : Loss: 0.1961, Train error: 0.3283, Train acc : 0.7046548956661315\n",
      "Val acc :0.32653061224489793\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.3105590062111801\n",
      "EPOCH  9\n",
      "Train : Loss: 0.1902, Train error: 0.2917, Train acc : 0.7299752270850537\n",
      "Val acc :0.5377358490566038\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.5024630541871921\n",
      "EPOCH  10\n",
      "Train : Loss: 0.1891, Train error: 0.2792, Train acc : 0.741962077493817\n",
      "Val acc :0.4692737430167598\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.4222222222222222\n",
      "EPOCH  11\n",
      "Train : Loss: 0.1823, Train error: 0.2935, Train acc : 0.7246861924686193\n",
      "Val acc :0.4644549763033175\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.5132743362831859\n",
      "EPOCH  12\n",
      "Train : Loss: 0.1845, Train error: 0.2810, Train acc : 0.7277441659464131\n",
      "Val acc :0.49411764705882355\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.36666666666666664\n",
      "EPOCH  13\n",
      "Train : Loss: 0.1779, Train error: 0.2694, Train acc : 0.7409948542024014\n",
      "Val acc :0.3255813953488372\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.388235294117647\n",
      "EPOCH  14\n",
      "Train : Loss: 0.1789, Train error: 0.2730, Train acc : 0.7362068965517241\n",
      "Val acc :0.4351851851851852\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.582995951417004\n",
      "EPOCH  15\n",
      "Train : Loss: 0.1717, Train error: 0.2685, Train acc : 0.7425149700598802\n",
      "Val acc :0.47297297297297297\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.2993197278911564\n",
      "EPOCH  16\n",
      "Train : Loss: 0.1694, Train error: 0.2623, Train acc : 0.747422680412371\n",
      "Val acc :0.4761904761904762\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.6979166666666667\n",
      "EPOCH  17\n",
      "Train : Loss: 0.1671, Train error: 0.2542, Train acc : 0.7510917030567685\n",
      "Val acc :0.4180790960451977\n",
      "Best validation accuracy  0.5625\n",
      "test acc :0.2958579881656805\n",
      "EPOCH  18\n",
      "Train : Loss: 0.1651, Train error: 0.2453, Train acc : 0.7551202137132679\n",
      "Val acc :0.5803571428571428\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.5803571428571428\n",
      "test acc :0.6050420168067226\n",
      "EPOCH  19\n",
      "Train : Loss: 0.1625, Train error: 0.2391, Train acc : 0.768566493955095\n",
      "Val acc :0.51\n",
      "Best validation accuracy  0.5803571428571428\n",
      "test acc :0.5972222222222222\n",
      "EPOCH  20\n",
      "Train : Loss: 0.1589, Train error: 0.2302, Train acc : 0.7764298093587522\n",
      "Val acc :0.6066350710900473\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.6066350710900473\n",
      "test acc :0.5814977973568283\n",
      "EPOCH  21\n",
      "Train : Loss: 0.1514, Train error: 0.2239, Train acc : 0.7878275570583263\n",
      "Val acc :0.6849315068493151\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.9222222222222223\n",
      "EPOCH  22\n",
      "Train : Loss: 0.1532, Train error: 0.2284, Train acc : 0.7815699658703072\n",
      "Val acc :0.3794871794871795\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.3636363636363636\n",
      "EPOCH  23\n",
      "Train : Loss: 0.1526, Train error: 0.2310, Train acc : 0.7810650887573964\n",
      "Val acc :0.5514018691588785\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.7979274611398963\n",
      "EPOCH  24\n",
      "Train : Loss: 0.1483, Train error: 0.2096, Train acc : 0.8010160880609654\n",
      "Val acc :0.6355140186915889\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.7272727272727273\n",
      "EPOCH  25\n",
      "Train : Loss: 0.1463, Train error: 0.2061, Train acc : 0.8030690537084398\n",
      "Val acc :0.5614035087719298\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.4262295081967213\n",
      "EPOCH  26\n",
      "Train : Loss: 0.1426, Train error: 0.1945, Train acc : 0.8164983164983164\n",
      "Val acc :0.5822784810126582\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.6926829268292684\n",
      "EPOCH  27\n",
      "Train : Loss: 0.1413, Train error: 0.2061, Train acc : 0.8047337278106508\n",
      "Val acc :0.6425339366515838\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.7722772277227722\n",
      "EPOCH  28\n",
      "Train : Loss: 0.1388, Train error: 0.1909, Train acc : 0.8228476821192053\n",
      "Val acc :0.42603550295857984\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.5333333333333333\n",
      "EPOCH  29\n",
      "Train : Loss: 0.1379, Train error: 0.1882, Train acc : 0.8201193520886616\n",
      "Val acc :0.6372549019607843\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.6878980891719745\n",
      "EPOCH  30\n",
      "Train : Loss: 0.1377, Train error: 0.1900, Train acc : 0.8214585079631183\n",
      "Val acc :0.6048780487804877\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.7241379310344827\n",
      "EPOCH  31\n",
      "Train : Loss: 0.1309, Train error: 0.1802, Train acc : 0.830820770519263\n",
      "Val acc :0.6504854368932038\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.7719298245614035\n",
      "EPOCH  32\n",
      "Train : Loss: 0.1274, Train error: 0.1802, Train acc : 0.8296795952782462\n",
      "Val acc :0.5730994152046784\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.21276595744680848\n",
      "EPOCH  33\n",
      "Train : Loss: 0.1306, Train error: 0.1713, Train acc : 0.8389261744966444\n",
      "Val acc :0.6372549019607843\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.4264705882352941\n",
      "EPOCH  34\n",
      "Train : Loss: 0.1289, Train error: 0.1909, Train acc : 0.8201680672268908\n",
      "Val acc :0.6107784431137725\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.35593220338983045\n",
      "EPOCH  35\n",
      "Train : Loss: 0.1198, Train error: 0.1820, Train acc : 0.8274111675126904\n",
      "Val acc :0.6346153846153846\n",
      "Best validation accuracy  0.6849315068493151\n",
      "test acc :0.7453416149068322\n",
      "EPOCH  36\n",
      "Train : Loss: 0.1282, Train error: 0.1793, Train acc : 0.8337468982630273\n",
      "Val acc :0.6862745098039217\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.6862745098039217\n",
      "test acc :0.8457142857142856\n",
      "EPOCH  37\n",
      "Train : Loss: 0.1264, Train error: 0.1731, Train acc : 0.8375209380234506\n",
      "Val acc :0.6701570680628273\n",
      "Best validation accuracy  0.6862745098039217\n",
      "test acc :0.31404958677685946\n",
      "EPOCH  38\n",
      "Train : Loss: 0.1188, Train error: 0.1748, Train acc : 0.8361204013377926\n",
      "Val acc :0.6850828729281768\n",
      "Best validation accuracy  0.6862745098039217\n",
      "test acc :0.4967320261437909\n",
      "EPOCH  39\n",
      "Train : Loss: 0.1204, Train error: 0.1668, Train acc : 0.8429890848026869\n",
      "Val acc :0.723404255319149\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.723404255319149\n",
      "test acc :0.822857142857143\n"
     ]
    }
   ],
   "source": [
    "best_model = \"./saved_models/best_model_MIL_raw_fs_MH_fir\"\n",
    "train_acc = []\n",
    "modelname=[]\n",
    "val_acc = []\n",
    "best_acc = 0\n",
    "for epoch in range(0, 40):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_error = 0.\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        bag_label = label[0]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "        train_loss += loss.item()\n",
    "        error,y_pred = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        train_error += error\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "        ypred.extend(y_pred.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    trainacc = f1_score(y,ypred)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_error /= len(train_loader)\n",
    "    print(\"EPOCH \",epoch)\n",
    "    print('Train : Loss: {:.4f}, Train error: {:.4f}, Train acc : {}'.format(train_loss, \n",
    "                                                                                train_error,trainacc))\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    model.eval()\n",
    "    for batch_idx, (data, label) in enumerate(val_loader):\n",
    "        bag_label = label[0]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        error, y_pred = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "        ypred.extend(y_pred.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "    valacc = f1_score(y,ypred)\n",
    "    print('Val acc :{}'.format(valacc))\n",
    "    if (valacc>=best_acc and trainacc>0.65):\n",
    "        print(\"---------State saved---------\")\n",
    "        best_acc = valacc\n",
    "        best_state=model.state_dict()\n",
    "        torch.save(best_state, best_model+'_epoch_'+str(epoch)+\".pth\")\n",
    "        modelname.append(best_model+'_epoch_'+str(epoch)+\".pth\")\n",
    "    print('Best validation accuracy ',best_acc)\n",
    "\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    for batch_idx, (data, label) in enumerate(test_loader):\n",
    "        bag_label = label[0]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        error, y_pred = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "        ypred.extend(y_pred.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "    valacc = f1_score(y,ypred)\n",
    "    print('test acc :{}'.format(valacc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8442211055276382 0.822857142857143 0.8470588235294118 0.8 0.8403669724770643 0.8807339449541285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4824120603015075 0.5502183406113537 0.45323741007194246 0.7 0.5013761467889908 0.30275229357798167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49748743718592964 0.34210526315789475 0.41935483870967744 0.28888888888888886 0.4793068297655453 0.6697247706422018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5276381909547738 0.6050420168067226 0.4864864864864865 0.8 0.5513761467889908 0.30275229357798167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5226130653266332 0.5814977973568283 0.48175182481751827 0.7333333333333333 0.5409785932721712 0.3486238532110092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9296482412060302 0.9222222222222223 0.9222222222222223 0.9222222222222223 0.9290010193679917 0.9357798165137615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864321608040201 0.8457142857142856 0.8705882352941177 0.8222222222222222 0.8606523955147808 0.8990825688073395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/home/hsharm04/.conda/envs/physio/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8442211055276382 0.822857142857143 0.8470588235294118 0.8 0.8403669724770643 0.8807339449541285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "device = torch.device(\"cuda\")\n",
    "model=Attention()\n",
    "# best_state=torch.load(\"../best-models/Attention_MIL_best(stress)_F1_91.pth\")\n",
    "best_state=torch.load(modelname[-1])\n",
    "model.load_state_dict(best_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test_loss = 0.\n",
    "correct = 0\n",
    "total = 0\n",
    "y =[]\n",
    "ypred = []\n",
    "test_error = 0.\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    bag_label = label[0]\n",
    "    instance_labels = label[1]\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    data, bag_label = Variable(data), Variable(bag_label)\n",
    "    loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "    test_loss += loss.item()\n",
    "    error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "    test_error += error\n",
    "    predicted_label=predicted_label.squeeze(1)\n",
    "    ypred.extend(predicted_label.tolist())\n",
    "    y.extend(bag_label.tolist())\n",
    "    \n",
    "acc=accuracy_score(y,ypred)\n",
    "# print(y,ypred)\n",
    "tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "f1score=f1_score(y,ypred)\n",
    "precision=precision_score(y,ypred)\n",
    "recall=recall_score(y,ypred)\n",
    "roc=roc_auc_score(y,ypred)\n",
    "specificity=tn/(tn+fp)\n",
    "print(acc,f1score,precision,recall,roc,specificity)\n",
    "\n",
    "for i in modelname:\n",
    "\n",
    "    model=Attention()\n",
    "    # best_state=torch.load(\"../best-models/Attention_MIL_best(stress)_F1_91.pth\")\n",
    "    best_state=torch.load(i)\n",
    "    model.load_state_dict(best_state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    test_error = 0.\n",
    "    for batch_idx, (data, label) in enumerate(test_loader):\n",
    "        bag_label = label[0]\n",
    "        instance_labels = label[1]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "        test_loss += loss.item()\n",
    "        error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        test_error += error\n",
    "        predicted_label=predicted_label.squeeze(1)\n",
    "        ypred.extend(predicted_label.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "\n",
    "    acc=accuracy_score(y,ypred)\n",
    "    # print(y,ypred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "    f1score=f1_score(y,ypred)\n",
    "    precision=precision_score(y,ypred)\n",
    "    recall=recall_score(y,ypred)\n",
    "    roc=roc_auc_score(y,ypred)\n",
    "    specificity=tn/(tn+fp)\n",
    "    print(acc,f1score,precision,recall,roc,specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./saved_models/best_model_MIL_raw_fs_MH_fir_epoch_39.pth'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (physio)",
   "language": "python",
   "name": "physio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
