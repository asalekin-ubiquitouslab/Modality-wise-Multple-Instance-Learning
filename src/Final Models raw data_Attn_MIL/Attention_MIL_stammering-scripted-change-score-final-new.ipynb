{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#2021, 3031,600,2455\n",
    "seed = 9427\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X1,Y):\n",
    "        self.X1 = X1\n",
    "        self.Y = Y\n",
    "    def __len__(self):        \n",
    "        return len(self.X1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X1[index]\n",
    "        y = self.Y[index]\n",
    "        return x,y\n",
    "\n",
    "#from model import Attention, GatedAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negetive pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def readFeatures(path):\n",
    "    features=np.array([])\n",
    "    start=0\n",
    "    for file in os.listdir(path):\n",
    "        d = os.path.join(path, file)\n",
    "        datafile=d+\"/features.npy\"\n",
    "        try:\n",
    "            temp=np.load(datafile)\n",
    "#             print(datafile)\n",
    "#             print(temp.shape)\n",
    "            if(start==0):\n",
    "                features=temp\n",
    "                if(d[-10]=='S'):\n",
    "                    label=np.ones(len(temp))\n",
    "                else:\n",
    "                    label=np.zeros(len(temp))\n",
    "                start=1\n",
    "            else:\n",
    "                features=np.vstack((features,temp))\n",
    "                if(d[-10]=='S'):\n",
    "                    label=np.append(label,np.ones(len(temp)))\n",
    "                else:\n",
    "                    label=np.append(label,np.zeros(len(temp)))\n",
    "        except IOError:\n",
    "            print('file not in Scripted')\n",
    "\n",
    "            \n",
    "    return features,label\n",
    "\n",
    "def load_data(path,mode):\n",
    "    if mode == \"test\":\n",
    "        xTest,yTest=readFeatures(path+\"/\"+mode)\n",
    "        xTest = xTest.reshape(len(xTest),1,19,8)\n",
    "        return Dataset(xTest,yTest)\n",
    "        \n",
    "    elif mode == \"train\":\n",
    "        xTrain,yTrain=readFeatures(path+\"/\"+mode)\n",
    "        xTrain = xTrain.reshape(len(xTrain),1,19,8)\n",
    "        return Dataset(xTrain,yTrain)\n",
    "        \n",
    "    elif mode == \"val\":\n",
    "        xVal,yVal=readFeatures(path+\"/\"+mode)\n",
    "        xVal = xVal.reshape(len(xVal),1,19,8)\n",
    "        return Dataset(xVal,yVal)\n",
    "    else:\n",
    "        print(\"Mode not defined\")\n",
    "        return\n",
    "\n",
    "traindataset = load_data(\"../Participant_wise_physio_change\",'train')\n",
    "testdataset = load_data(\"../Participant_wise_physio_change\",'test')\n",
    "valdataset = load_data(\"../Participant_wise_physio_change\",'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain=np.load(\"../extracted_features/Change/input_features_train_grp3.npy\")\n",
    "# labels_stress_train=np.load(\"../extracted_features/Change/labels_stress_train_grp3.npy\")\n",
    "# yTrain=np.load(\"../extracted_features/Change/labels_train_grp3.npy\")\n",
    "\n",
    "# xTest=np.load(\"../extracted_features/Change/input_features_test_grp3.npy\")\n",
    "# labels_stress_test=np.load(\"../extracted_features/Change/labels_stress_test_grp3.npy\")\n",
    "# yTest=np.load(\"../extracted_features/Change/labels_test_grp3.npy\")\n",
    "\n",
    "\n",
    "# xVal=np.load(\"../extracted_features/Change/input_features_val_grp3.npy\")\n",
    "# labels_stress_val=np.load(\"../extracted_features/Change/labels_stress_val_grp3.npy\")\n",
    "# yVal=np.load(\"../extracted_features/Change/labels_val_grp3.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain = xTrain.reshape(len(xTrain),1,19,8)\n",
    "# xTest = xTest.reshape(len(xTest),1,19,8)\n",
    "# xVal = xVal.reshape(len(xVal),1,19,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xTrain.shape,yTrain.shape)\n",
    "# print(xTest.shape,yTest.shape)\n",
    "# print(xVal.shape,yVal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindataset = Dataset(xTrain,yTrain)\n",
    "# testdataset = Dataset(xTest,yTest)\n",
    "# valdataset = Dataset(xVal,yVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stammering(data_utils.Dataset):\n",
    "    def __init__(self, target_number=1, mean_bag_length=5, var_bag_length=2, num_bag=150, seed=2021, train=\"train\"):\n",
    "        self.target_number = target_number\n",
    "        self.mean_bag_length = mean_bag_length\n",
    "        self.var_bag_length = var_bag_length\n",
    "        self.num_bag = num_bag\n",
    "        self.train = train\n",
    "        self.r = np.random.RandomState(seed)\n",
    "\n",
    "\n",
    "\n",
    "        if self.train==\"train\":\n",
    "            self.train_bags_list, self.train_labels_list = self._create_bags()\n",
    "        elif self.train==\"val\":\n",
    "            self.val_bags_list, self.val_labels_list = self._create_bags()\n",
    "        else:\n",
    "            self.test_bags_list, self.test_labels_list = self._create_bags()\n",
    "\n",
    "    def _create_bags(self):\n",
    "        if self.train==\"train\":\n",
    "            print(\"train\")\n",
    "            loader = data_utils.DataLoader(traindataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "        elif self.train==\"val\":\n",
    "            print(\"val\")\n",
    "            loader = data_utils.DataLoader(valdataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "        else:\n",
    "            loader = data_utils.DataLoader(testdataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "            \n",
    "        bags_list = []\n",
    "        labels_list = []\n",
    "        for (batch_data, batch_labels) in loader:\n",
    "            #print(batch_data.shape)\n",
    "            bags_list.append(batch_data.reshape(19,1,8))\n",
    "            temp = torch.as_tensor(np.array([batch_labels for x in range(19)]))\n",
    "            labels_list.append(temp)\n",
    "            \n",
    "               \n",
    "        #print(bags_list)\n",
    "        #print(labels_list)\n",
    "        return bags_list, labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train==\"train\":\n",
    "            return len(self.train_labels_list)\n",
    "        elif self.train==\"val\":\n",
    "            return len(self.val_labels_list)\n",
    "        else:\n",
    "            return len(self.test_labels_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train==\"train\":\n",
    "            bag = self.train_bags_list[index]\n",
    "            label = [max(self.train_labels_list[index]), self.train_labels_list[index]]\n",
    "        elif self.train==\"val\":\n",
    "            bag = self.val_bags_list[index]\n",
    "            label = [max(self.val_labels_list[index]), self.val_labels_list[index]]\n",
    "        else:\n",
    "            bag = self.test_bags_list[index]\n",
    "            label = [max(self.test_labels_list[index]), self.test_labels_list[index]]\n",
    "\n",
    "        return bag, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "train_loader = data_utils.DataLoader(Stammering(train=\"train\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)\n",
    "test_loader = data_utils.DataLoader(Stammering(train=\"test\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)\n",
    "val_loader = data_utils.DataLoader(Stammering(train=\"val\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.L = 256\n",
    "        self.D = 64\n",
    "        self.K = 1\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 128, kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 64, kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256,self.L),\n",
    "            nn.ReLU()\n",
    "\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "\n",
    "            nn.Linear(self.L*self.K, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        \n",
    "        #H = H.view(-1, 256*5*4)\n",
    "        H = H.view(H.size(0), -1)\n",
    "        #print(H.shape)\n",
    "        H = self.feature_extractor_part2(H)  # NxL\n",
    "\n",
    "        A = self.attention(H)  # NxK\n",
    "\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "\n",
    "        A = F.softmax(A, dim=1)  # softmax over N\n",
    "\n",
    "        M = torch.mm(A, H)  # KxL\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob\n",
    "\n",
    "\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob= self.forward(X)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model.cuda(),(1,8))\n",
    "# # torch.save(model, \"modelarchitect_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH  0\n",
      "Train : Loss: 0.6898, Train error: 0.4612, Train acc : 0.0\n",
      "Val : Loss: 0.6919, val error: 0.4746, Val acc :0.06666666666666667\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.06666666666666667\n",
      "test :test error: 0.4576, Val acc :0.18181818181818182\n",
      "EPOCH  1\n",
      "Train : Loss: 0.6779, Train error: 0.4360, Train acc : 0.3119266055045871\n",
      "Val : Loss: 0.7675, val error: 0.6102, Val acc :0.0\n",
      "Best validation accuracy  0.06666666666666667\n",
      "test :test error: 0.4576, Val acc :0.2058823529411765\n",
      "EPOCH  2\n",
      "Train : Loss: 0.6705, Train error: 0.3973, Train acc : 0.46475195822454307\n",
      "Val : Loss: 0.7677, val error: 0.5932, Val acc :0.2708333333333333\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.2708333333333333\n",
      "test :test error: 0.4068, Val acc :0.5555555555555556\n",
      "EPOCH  3\n",
      "Train : Loss: 0.6559, Train error: 0.3682, Train acc : 0.5497630331753555\n",
      "Val : Loss: 0.7916, val error: 0.5339, Val acc :0.5190839694656488\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.5190839694656488\n",
      "test :test error: 0.3220, Val acc :0.6481481481481481\n",
      "EPOCH  4\n",
      "Train : Loss: 0.6443, Train error: 0.3663, Train acc : 0.608695652173913\n",
      "Val : Loss: 0.7968, val error: 0.5085, Val acc :0.5652173913043478\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.5652173913043478\n",
      "test :test error: 0.3559, Val acc :0.671875\n",
      "EPOCH  5\n",
      "Train : Loss: 0.6220, Train error: 0.3643, Train acc : 0.5859030837004405\n",
      "Val : Loss: 0.9342, val error: 0.5508, Val acc :0.5911949685534591\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.5911949685534591\n",
      "test :test error: 0.3136, Val acc :0.7482993197278911\n",
      "EPOCH  6\n",
      "Train : Loss: 0.6157, Train error: 0.3411, Train acc : 0.6317991631799162\n",
      "Val : Loss: 1.0557, val error: 0.5508, Val acc :0.48\n",
      "Best validation accuracy  0.5911949685534591\n",
      "test :test error: 0.3559, Val acc :0.58\n",
      "EPOCH  7\n",
      "Train : Loss: 0.6229, Train error: 0.3624, Train acc : 0.6046511627906977\n",
      "Val : Loss: 1.1816, val error: 0.4746, Val acc :0.5625\n",
      "Best validation accuracy  0.5911949685534591\n",
      "test :test error: 0.3898, Val acc :0.5660377358490567\n",
      "EPOCH  8\n",
      "Train : Loss: 0.6148, Train error: 0.3391, Train acc : 0.6220302375809935\n",
      "Val : Loss: 1.5386, val error: 0.5085, Val acc :0.625\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3136, Val acc :0.7549668874172185\n",
      "EPOCH  9\n",
      "Train : Loss: 0.6075, Train error: 0.3430, Train acc : 0.6226012793176973\n",
      "Val : Loss: 1.5728, val error: 0.5508, Val acc :0.5637583892617449\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3136, Val acc :0.7259259259259259\n",
      "EPOCH  10\n",
      "Train : Loss: 0.5948, Train error: 0.3256, Train acc : 0.6331877729257642\n",
      "Val : Loss: 1.5414, val error: 0.5339, Val acc :0.5333333333333333\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3729, Val acc :0.6140350877192983\n",
      "EPOCH  11\n",
      "Train : Loss: 0.5923, Train error: 0.3314, Train acc : 0.6157303370786517\n",
      "Val : Loss: 1.6249, val error: 0.4831, Val acc :0.544\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3729, Val acc :0.5849056603773585\n",
      "EPOCH  12\n",
      "Train : Loss: 0.5805, Train error: 0.3295, Train acc : 0.6458333333333333\n",
      "Val : Loss: 1.4179, val error: 0.4407, Val acc :0.5666666666666665\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3814, Val acc :0.536082474226804\n",
      "EPOCH  13\n",
      "Train : Loss: 0.5768, Train error: 0.3450, Train acc : 0.6079295154185022\n",
      "Val : Loss: 1.4186, val error: 0.5085, Val acc :0.5454545454545455\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3729, Val acc :0.576923076923077\n",
      "EPOCH  14\n",
      "Train : Loss: 0.5698, Train error: 0.3004, Train acc : 0.6608315098468271\n",
      "Val : Loss: 1.8394, val error: 0.5424, Val acc :0.5223880597014925\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3559, Val acc :0.6037735849056604\n",
      "EPOCH  15\n",
      "Train : Loss: 0.5599, Train error: 0.2907, Train acc : 0.6781115879828327\n",
      "Val : Loss: 2.2269, val error: 0.5169, Val acc :0.5547445255474452\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3475, Val acc :0.6238532110091743\n",
      "EPOCH  16\n",
      "Train : Loss: 0.5600, Train error: 0.2965, Train acc : 0.6709677419354839\n",
      "Val : Loss: 2.2154, val error: 0.5085, Val acc :0.53125\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3814, Val acc :0.5454545454545454\n",
      "EPOCH  17\n",
      "Train : Loss: 0.5326, Train error: 0.2926, Train acc : 0.6606741573033709\n",
      "Val : Loss: 2.7929, val error: 0.4831, Val acc :0.5777777777777778\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3305, Val acc :0.688\n",
      "EPOCH  18\n",
      "Train : Loss: 0.5434, Train error: 0.3140, Train acc : 0.6478260869565218\n",
      "Val : Loss: 1.8311, val error: 0.5339, Val acc :0.4615384615384615\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3559, Val acc :0.5227272727272728\n",
      "EPOCH  19\n",
      "Train : Loss: 0.5452, Train error: 0.2733, Train acc : 0.6873614190687362\n",
      "Val : Loss: 1.5642, val error: 0.5085, Val acc :0.411764705882353\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3644, Val acc :0.44155844155844154\n",
      "EPOCH  20\n",
      "Train : Loss: 0.5294, Train error: 0.2907, Train acc : 0.6666666666666666\n",
      "Val : Loss: 3.2707, val error: 0.5424, Val acc :0.536231884057971\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2881, Val acc :0.711864406779661\n",
      "EPOCH  21\n",
      "Train : Loss: 0.5173, Train error: 0.2539, Train acc : 0.7095343680709534\n",
      "Val : Loss: 2.4442, val error: 0.5932, Val acc :0.4444444444444445\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2627, Val acc :0.7155963302752292\n",
      "EPOCH  22\n",
      "Train : Loss: 0.5280, Train error: 0.2926, Train acc : 0.6606741573033709\n",
      "Val : Loss: 3.9246, val error: 0.6356, Val acc :0.48979591836734687\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2119, Val acc :0.7999999999999999\n",
      "EPOCH  23\n",
      "Train : Loss: 0.5333, Train error: 0.2713, Train acc : 0.6846846846846846\n",
      "Val : Loss: 3.0632, val error: 0.5254, Val acc :0.5373134328358208\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3305, Val acc :0.6213592233009709\n",
      "EPOCH  24\n",
      "Train : Loss: 0.5039, Train error: 0.2694, Train acc : 0.6789838337182448\n",
      "Val : Loss: 4.0761, val error: 0.5508, Val acc :0.5255474452554744\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2966, Val acc :0.6534653465346535\n",
      "EPOCH  25\n",
      "Train : Loss: 0.4931, Train error: 0.2384, Train acc : 0.7146171693735499\n",
      "Val : Loss: 3.2946, val error: 0.5847, Val acc :0.47328244274809156\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2966, Val acc :0.6666666666666665\n",
      "EPOCH  26\n",
      "Train : Loss: 0.4802, Train error: 0.2481, Train acc : 0.7104072398190046\n",
      "Val : Loss: 2.7294, val error: 0.5169, Val acc :0.5196850393700788\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3475, Val acc :0.6095238095238096\n",
      "EPOCH  27\n",
      "Train : Loss: 0.5142, Train error: 0.2597, Train acc : 0.6869158878504673\n",
      "Val : Loss: 2.1675, val error: 0.6356, Val acc :0.358974358974359\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3136, Val acc :0.5747126436781608\n",
      "EPOCH  28\n",
      "Train : Loss: 0.5034, Train error: 0.2655, Train acc : 0.6893424036281179\n",
      "Val : Loss: 1.5262, val error: 0.5424, Val acc :0.38461538461538464\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3644, Val acc :0.5168539325842697\n",
      "EPOCH  29\n",
      "Train : Loss: 0.4635, Train error: 0.2636, Train acc : 0.6866359447004609\n",
      "Val : Loss: 4.2050, val error: 0.5847, Val acc :0.543046357615894\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2797, Val acc :0.7441860465116279\n",
      "EPOCH  30\n",
      "Train : Loss: 0.4644, Train error: 0.2500, Train acc : 0.6964705882352941\n",
      "Val : Loss: 1.7141, val error: 0.4831, Val acc :0.42424242424242425\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3898, Val acc :0.4102564102564103\n",
      "EPOCH  31\n",
      "Train : Loss: 0.4716, Train error: 0.2597, Train acc : 0.6981981981981983\n",
      "Val : Loss: 4.2029, val error: 0.5000, Val acc :0.609271523178808\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2881, Val acc :0.7424242424242424\n",
      "EPOCH  32\n",
      "Train : Loss: 0.4791, Train error: 0.2461, Train acc : 0.7133182844243792\n",
      "Val : Loss: 4.3088, val error: 0.6271, Val acc :0.5\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2373, Val acc :0.787878787878788\n",
      "EPOCH  33\n",
      "Train : Loss: 0.4404, Train error: 0.2112, Train acc : 0.755056179775281\n",
      "Val : Loss: 3.5738, val error: 0.5424, Val acc :0.5555555555555556\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3220, Val acc :0.6779661016949153\n",
      "EPOCH  34\n",
      "Train : Loss: 0.4717, Train error: 0.2345, Train acc : 0.734065934065934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val : Loss: 3.4377, val error: 0.5000, Val acc :0.5874125874125875\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3305, Val acc :0.7022900763358779\n",
      "EPOCH  35\n",
      "Train : Loss: 0.4500, Train error: 0.2267, Train acc : 0.7370786516853932\n",
      "Val : Loss: 3.0005, val error: 0.5932, Val acc :0.43548387096774194\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2627, Val acc :0.6804123711340206\n",
      "EPOCH  36\n",
      "Train : Loss: 0.4543, Train error: 0.2190, Train acc : 0.7378190255220417\n",
      "Val : Loss: 3.1868, val error: 0.5424, Val acc :0.5294117647058824\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3051, Val acc :0.7\n",
      "EPOCH  37\n",
      "Train : Loss: 0.4365, Train error: 0.2074, Train acc : 0.7482352941176471\n",
      "Val : Loss: 3.6075, val error: 0.4661, Val acc :0.5985401459854014\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3390, Val acc :0.696969696969697\n",
      "EPOCH  38\n",
      "Train : Loss: 0.4493, Train error: 0.1957, Train acc : 0.7688787185354692\n",
      "Val : Loss: 1.6788, val error: 0.4492, Val acc :0.41758241758241754\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3136, Val acc :0.6185567010309279\n",
      "EPOCH  39\n",
      "Train : Loss: 0.4305, Train error: 0.2035, Train acc : 0.7671840354767183\n",
      "Val : Loss: 1.7300, val error: 0.4831, Val acc :0.544\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2966, Val acc :0.7058823529411765\n",
      "EPOCH  40\n",
      "Train : Loss: 0.4237, Train error: 0.2171, Train acc : 0.7442922374429224\n",
      "Val : Loss: 3.8527, val error: 0.5085, Val acc :0.5945945945945945\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3729, Val acc :0.6901408450704226\n",
      "EPOCH  41\n",
      "Train : Loss: 0.4391, Train error: 0.2171, Train acc : 0.7488789237668162\n",
      "Val : Loss: 2.7248, val error: 0.5000, Val acc :0.5874125874125875\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3136, Val acc :0.7040000000000001\n",
      "EPOCH  42\n",
      "Train : Loss: 0.4108, Train error: 0.1977, Train acc : 0.7713004484304934\n",
      "Val : Loss: 3.6982, val error: 0.5085, Val acc :0.6\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3475, Val acc :0.7092198581560283\n",
      "EPOCH  43\n",
      "Train : Loss: 0.3960, Train error: 0.1919, Train acc : 0.7824175824175823\n",
      "Val : Loss: 3.1841, val error: 0.4915, Val acc :0.591549295774648\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.2881, Val acc :0.746268656716418\n",
      "EPOCH  44\n",
      "Train : Loss: 0.4170, Train error: 0.1996, Train acc : 0.7621247113163974\n",
      "Val : Loss: 2.3409, val error: 0.5254, Val acc :0.507936507936508\n",
      "Best validation accuracy  0.625\n",
      "test :test error: 0.3475, Val acc :0.7092198581560283\n"
     ]
    }
   ],
   "source": [
    "model = Attention()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "if torch.cuda.is_available():model.cuda()\n",
    "best_model = \"./saved_models/best_model_Attention-mil-scripted-change-grp3\"\n",
    "train_acc = []\n",
    "modelname=[]\n",
    "val_acc = []\n",
    "best_acc = 0\n",
    "for epoch in range(0, 45):\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_error = 0.\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        bag_label = label[0]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "        train_loss += loss.data[0]\n",
    "        error,y_pred = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        train_error += error\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "        ypred.extend(y_pred.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    trainacc = f1_score(y,ypred)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_error /= len(train_loader)\n",
    "    print(\"EPOCH \",epoch)\n",
    "    print('Train : Loss: {:.4f}, Train error: {:.4f}, Train acc : {}'.format(train_loss.cpu().numpy()[0], \n",
    "                                                                                train_error,trainacc))\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    val_error=0.\n",
    "    val_loss= 0.\n",
    "    model.eval()\n",
    "    for batch_idx, (data, label) in enumerate(val_loader):\n",
    "        bag_label = label[0]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "        val_loss += loss.data[0]\n",
    "        error, y_pred = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        val_error += error\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "\n",
    "        ypred.extend(y_pred.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "    valacc = f1_score(y,ypred)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_error /= len(val_loader)\n",
    "    print('Val : Loss: {:.4f}, val error: {:.4f}, Val acc :{}'.format(val_loss.cpu().numpy()[0], val_error,valacc))\n",
    "    if valacc>=best_acc:\n",
    "        print(\"---------State saved---------\")\n",
    "        best_acc = valacc\n",
    "        best_state=model.state_dict()\n",
    "        torch.save(best_state, best_model+'_epoch_'+str(epoch)+\".pth\")\n",
    "        modelname.append(best_model+'_epoch_'+str(epoch)+\".pth\")\n",
    "    print('Best validation accuracy ',best_acc)\n",
    "    test_loss = 0.\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    ypred1=[]\n",
    "    test_error = 0.\n",
    "    for batch_idx, (data, label) in enumerate(test_loader):\n",
    "        bag_label = label[0]\n",
    "        instance_labels = label[1]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        y_pred = model(data.float())\n",
    "        error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        test_error += error\n",
    "        predicted_label=predicted_label.squeeze(1)\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "        ypred.extend(predicted_label.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "    testacc = f1_score(y,ypred)\n",
    "    test_error /= len(test_loader)\n",
    "    print('test :test error: {:.4f}, Val acc :{}'.format(test_error,testacc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652542372881356 0.7092198581560283 0.6024096385542169 0.8620689655172413 0.6560344827586206 0.45\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model=Attention()\n",
    "# best_state=torch.load(\"../best-models/Attention_MIL_scripted_change_F1_79.pth\")\n",
    "model.load_state_dict(best_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test_loss = 0.\n",
    "correct = 0\n",
    "total = 0\n",
    "y =[]\n",
    "ypred = []\n",
    "ypred1=[]\n",
    "test_error = 0.\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    bag_label = label[0]\n",
    "    instance_labels = label[1]\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    data, bag_label = Variable(data), Variable(bag_label)\n",
    "    y_pred = model(data.float())\n",
    "    error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "    test_error += error\n",
    "    predicted_label=predicted_label.squeeze(1)\n",
    "    y_pred=y_pred.squeeze(1)\n",
    "    ypred.extend(predicted_label.tolist())\n",
    "    ypred1.extend(y_pred.tolist())\n",
    "    y.extend(bag_label.tolist())\n",
    "    \n",
    "acc=accuracy_score(y,ypred)\n",
    "# print(y,ypred)\n",
    "tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "f1score=f1_score(y,ypred)\n",
    "precision=precision_score(y,ypred)\n",
    "recall=recall_score(y,ypred)\n",
    "roc=roc_auc_score(y,ypred)\n",
    "specificity=tn/(tn+fp)\n",
    "print(acc,f1score,precision,recall,roc,specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window1 and Window-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6440677966101694 0.7272727272727274 0.5833333333333334 0.9655172413793104 0.6494252873563219 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "testdataset = load_data(\"../Participant_wise_physio_change1\",'test')\n",
    "test_loader = data_utils.DataLoader(Stammering(train=\"test\"),worker_init_fn=seed_worker,batch_size=1,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model=Attention()\n",
    "# best_state=torch.load(\"../best-models/Attention_MIL_scripted_change_F1_79.pth\")\n",
    "best_state=torch.load(modelname[-1])\n",
    "model.load_state_dict(best_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test_loss = 0.\n",
    "correct = 0\n",
    "total = 0\n",
    "y =[]\n",
    "ypred = []\n",
    "test_error = 0.\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    bag_label = label[0]\n",
    "    instance_labels = label[1]\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    data, bag_label = Variable(data), Variable(bag_label)\n",
    "    loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "    test_loss += loss.data[0]\n",
    "    error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "    test_error += error\n",
    "    predicted_label=predicted_label.squeeze(1)\n",
    "    ypred.extend(predicted_label.tolist())\n",
    "    y.extend(bag_label.tolist())\n",
    "    \n",
    "acc=accuracy_score(y,ypred)\n",
    "# print(y,ypred)\n",
    "tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "f1score=f1_score(y,ypred)\n",
    "precision=precision_score(y,ypred)\n",
    "recall=recall_score(y,ypred)\n",
    "roc=roc_auc_score(y,ypred)\n",
    "specificity=tn/(tn+fp)\n",
    "print(acc,f1score,precision,recall,roc,specificity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7288135593220338 0.7837837837837839 0.6444444444444445 1.0 0.7333333333333334 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "testdataset = load_data(\"../Participant_wise_physio_change2\",'test')\n",
    "test_loader = data_utils.DataLoader(Stammering(train=\"test\"),worker_init_fn=seed_worker,batch_size=1,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model=Attention()\n",
    "# best_state=torch.load(\"../best-models/Attention_MIL_scripted_change_F1_79.pth\")\n",
    "best_state=torch.load(modelname[-1])\n",
    "model.load_state_dict(best_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test_loss = 0.\n",
    "correct = 0\n",
    "total = 0\n",
    "y =[]\n",
    "ypred = []\n",
    "test_error = 0.\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    bag_label = label[0]\n",
    "    instance_labels = label[1]\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    data, bag_label = Variable(data), Variable(bag_label)\n",
    "    loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "    test_loss += loss.data[0]\n",
    "    error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "    test_error += error\n",
    "    predicted_label=predicted_label.squeeze(1)\n",
    "    ypred.extend(predicted_label.tolist())\n",
    "    y.extend(bag_label.tolist())\n",
    "    \n",
    "acc=accuracy_score(y,ypred)\n",
    "# print(y,ypred)\n",
    "tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "f1score=f1_score(y,ypred)\n",
    "precision=precision_score(y,ypred)\n",
    "recall=recall_score(y,ypred)\n",
    "roc=roc_auc_score(y,ypred)\n",
    "specificity=tn/(tn+fp)\n",
    "print(acc,f1score,precision,recall,roc,specificity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_models/best_model_Attention-mil-scripted-change-grp3_epoch_0.pth\n",
      "0.576271186440678 0.2424242424242424 1.0 0.13793103448275862 0.5689655172413793 1.0\n",
      "./saved_models/best_model_Attention-mil-scripted-change-grp3_epoch_2.pth\n",
      "0.6271186440677966 0.5925925925925927 0.64 0.5517241379310345 0.6258620689655172 0.7\n",
      "./saved_models/best_model_Attention-mil-scripted-change-grp3_epoch_3.pth\n",
      "0.6949152542372882 0.6538461538461539 0.7391304347826086 0.5862068965517241 0.6931034482758621 0.8\n",
      "./saved_models/best_model_Attention-mil-scripted-change-grp3_epoch_4.pth\n",
      "0.6610169491525424 0.6666666666666667 0.6451612903225806 0.6896551724137931 0.6614942528735632 0.6333333333333333\n",
      "./saved_models/best_model_Attention-mil-scripted-change-grp3_epoch_5.pth\n",
      "0.711864406779661 0.7605633802816901 0.6428571428571429 0.9310344827586207 0.7155172413793103 0.5\n",
      "./saved_models/best_model_Attention-mil-scripted-change-grp3_epoch_8.pth\n",
      "0.7288135593220338 0.7837837837837839 0.6444444444444445 1.0 0.7333333333333334 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(modelname)):\n",
    "#     print(modelname[i])\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model=Attention()\n",
    "#     # best_state=torch.load(\"../best-models/Attention_MIL_scripted_change_F1_79.pth\")\n",
    "#     best_state=torch.load(modelname[i])\n",
    "#     model.load_state_dict(best_state)\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     test_loss = 0.\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     y =[]\n",
    "#     ypred = []\n",
    "#     test_error = 0.\n",
    "#     for batch_idx, (data, label) in enumerate(test_loader):\n",
    "#         bag_label = label[0]\n",
    "#         instance_labels = label[1]\n",
    "#         data, bag_label = data.cuda(), bag_label.cuda()\n",
    "#         data, bag_label = Variable(data), Variable(bag_label)\n",
    "#         loss = model.calculate_objective(data.float(), bag_label.float())\n",
    "#         test_loss += loss.data[0]\n",
    "#         error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "#         test_error += error\n",
    "#         predicted_label=predicted_label.squeeze(1)\n",
    "#         ypred.extend(predicted_label.tolist())\n",
    "#         y.extend(bag_label.tolist())\n",
    "\n",
    "#     acc=accuracy_score(y,ypred)\n",
    "#     # print(y,ypred)\n",
    "#     tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "#     f1score=f1_score(y,ypred)\n",
    "#     precision=precision_score(y,ypred)\n",
    "#     recall=recall_score(y,ypred)\n",
    "#     roc=roc_auc_score(y,ypred)\n",
    "#     specificity=tn/(tn+fp)\n",
    "#     print(acc,f1score,precision,recall,roc,specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (physio)",
   "language": "python",
   "name": "physio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
