{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "seed = 2021\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    \n",
    "    \n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X1,Y):\n",
    "        self.X1 = X1\n",
    "        self.Y = Y\n",
    "    def __len__(self):        \n",
    "        return len(self.X1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X1[index]\n",
    "        y = self.Y[index]\n",
    "        return x,y\n",
    "\n",
    "#from model import Attention, GatedAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negetive pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def readFeatures(path):\n",
    "    features=np.array([])\n",
    "    start=0\n",
    "    for file in os.listdir(path):\n",
    "        d = os.path.join(path, file)\n",
    "        datafile=d+\"/features.npy\"\n",
    "        try:\n",
    "            temp=np.load(datafile)\n",
    "#             print(datafile)\n",
    "#             print(temp.shape)\n",
    "            if(start==0):\n",
    "                features=temp\n",
    "                if(d[-10]=='S'):\n",
    "                    label=np.ones(len(temp))\n",
    "                else:\n",
    "                    label=np.zeros(len(temp))\n",
    "                start=1\n",
    "            else:\n",
    "                features=np.vstack((features,temp))\n",
    "                if(d[-10]=='S'):\n",
    "                    label=np.append(label,np.ones(len(temp)))\n",
    "                else:\n",
    "                    label=np.append(label,np.zeros(len(temp)))\n",
    "        except IOError:\n",
    "            print('file not in Scripted')\n",
    "\n",
    "            \n",
    "    return features,label\n",
    "\n",
    "def load_data(path,mode):\n",
    "    if mode == \"test\":\n",
    "        xTest,yTest=readFeatures(path+\"/\"+mode)\n",
    "        xTest = xTest.reshape(len(xTest),1,19,24)\n",
    "        return Dataset(xTest,yTest)\n",
    "        \n",
    "    elif mode == \"train\":\n",
    "        xTrain,yTrain=readFeatures(path+\"/\"+mode)\n",
    "        xTrain = xTrain.reshape(len(xTrain),1,19,24)\n",
    "        return Dataset(xTrain,yTrain)\n",
    "        \n",
    "    elif mode == \"val\":\n",
    "        xVal,yVal=readFeatures(path+\"/\"+mode)\n",
    "        xVal = xVal.reshape(len(xVal),1,19,24)\n",
    "        return Dataset(xVal,yVal)\n",
    "    else:\n",
    "        print(\"Mode not defined\")\n",
    "        return\n",
    "\n",
    "traindataset = load_data(\"../Participant_wise\",'train')\n",
    "testdataset = load_data('../Participant_wise','test')\n",
    "valdataset = load_data('../Participant_wise','val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain=np.load(\"../extracted_features/0/GRP-3/input_features_train_grp3.npy\")\n",
    "# labels_stress_train=np.load(\"../extracted_features/0/GRP-3/labels_stress_train_grp3.npy\")\n",
    "# yTrain=np.load(\"../extracted_features/0/GRP-3/labels_train_grp3.npy\")\n",
    "\n",
    "# xTest=np.load(\"../extracted_features/0/GRP-3/input_features_test_grp3.npy\")\n",
    "# labels_stress_test=np.load(\"../extracted_features/0/GRP-3/labels_stress_test_grp3.npy\")\n",
    "# yTest=np.load(\"../extracted_features/0/GRP-3/labels_test_grp3.npy\")\n",
    "\n",
    "\n",
    "# xVal=np.load(\"../extracted_features/0/GRP-3/input_features_val_grp3.npy\")\n",
    "# labels_stress_val=np.load(\"../extracted_features/0/GRP-3/labels_stress_val_grp3.npy\")\n",
    "# yVal=np.load(\"../extracted_features/0/GRP-3/labels_val_grp3.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features=np.load(\"../extracted_features/bl/0/GRP-3/input_features_train_grp3.npy\")\n",
    "# labels_stress_train=np.load(\"../extracted_features/bl/0/GRP-3/labels_stress_train_grp3.npy\")\n",
    "# labels_train=np.load(\"../extracted_features/bl/0/GRP-3/labels_train_grp3.npy\")\n",
    "\n",
    "# test_features=np.load(\"../extracted_features/bl/0/GRP-3/input_features_test_grp3.npy\")\n",
    "# labels_stress_test=np.load(\"../extracted_features/bl/0/GRP-3/labels_stress_test_grp3.npy\")\n",
    "# labels_test=np.load(\"../extracted_features/bl/0/GRP-3/labels_test_grp3.npy\")\n",
    "\n",
    "# val_features=np.load(\"../extracted_features/bl/0/GRP-3/input_features_val_grp3.npy\")\n",
    "# labels_stress_val=np.load(\"../extracted_features/bl/0/GRP-3/labels_stress_val_grp3.npy\")\n",
    "# labels_val=np.load(\"../extracted_features/bl/0/GRP-3/labels_val_grp3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497, 19, 24) (497,)\n",
      "(119, 19, 24) (119,)\n",
      "(110, 19, 24) (110,)\n"
     ]
    }
   ],
   "source": [
    "# print(train_features.shape,labels_train.shape)\n",
    "# print(test_features.shape,labels_test.shape)\n",
    "# print(val_features.shape,labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features=train_features.reshape(len(train_features),1,19,24)\n",
    "# test_features=test_features.reshape(len(test_features),1,19,24)\n",
    "# val_features=val_features.reshape(len(val_features),1,19,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain = xTrain.reshape(len(xTrain),1,19,24)\n",
    "# xTest = xTest.reshape(len(xTest),1,19,24)\n",
    "# xVal = xVal.reshape(len(xVal),1,19,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xTrain=np.append(train_features,xTrain,axis=0)\n",
    "# xTest=np.append(test_features,xTest,axis=0)\n",
    "# xVal=np.append(val_features,xVal,axis=0)\n",
    "# yTrain=np.append(labels_train,yTrain,axis=0)\n",
    "# yTest=np.append(labels_test,yTest,axis=0)\n",
    "# yVal=np.append(labels_val,yVal,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xTrain.shape,yTrain.shape)\n",
    "# print(xTest.shape,yTest.shape)\n",
    "# print(xVal.shape,yVal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindataset = Dataset(train_features,labels_train)\n",
    "# testdataset = Dataset(test_features,labels_test)\n",
    "# valdataset = Dataset(val_features,labels_val)\n",
    "\n",
    "# traindataset = Dataset(xTrain,yTrain)\n",
    "# testdataset = Dataset(xTest,yTest)\n",
    "# valdataset = Dataset(xVal,yVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stammering(data_utils.Dataset):\n",
    "    def __init__(self, target_number=1, mean_bag_length=5, var_bag_length=2, num_bag=150, seed=2021, train=\"train\"):\n",
    "        self.target_number = target_number\n",
    "        self.mean_bag_length = mean_bag_length\n",
    "        self.var_bag_length = var_bag_length\n",
    "        self.num_bag = num_bag\n",
    "        self.train = train\n",
    "        self.r = np.random.RandomState(seed)\n",
    "\n",
    "        self.num_in_train = 512\n",
    "        self.num_in_test = 122\n",
    "        self.num_in_val=118\n",
    "\n",
    "        if self.train==\"train\":\n",
    "            self.train_bags_list, self.train_labels_list = self._create_bags()\n",
    "        elif self.train==\"val\":\n",
    "            self.val_bags_list, self.val_labels_list = self._create_bags()\n",
    "        else:\n",
    "            self.test_bags_list, self.test_labels_list = self._create_bags()\n",
    "\n",
    "    def _create_bags(self):\n",
    "        if self.train==\"train\":\n",
    "            print(\"train\")\n",
    "            loader = data_utils.DataLoader(traindataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "        elif self.train==\"val\":\n",
    "            print(\"val\")\n",
    "            loader = data_utils.DataLoader(valdataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "        else:\n",
    "            loader = data_utils.DataLoader(testdataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "            \n",
    "        bags_list = []\n",
    "        labels_list = []\n",
    "        for (batch_data, batch_labels) in loader:\n",
    "            #print(batch_data.shape)\n",
    "            bags_list.append(batch_data.reshape(19,1,24))\n",
    "            temp = torch.as_tensor(np.array([batch_labels for x in range(19)]))\n",
    "            labels_list.append(temp)\n",
    "            \n",
    "               \n",
    "        #print(bags_list)\n",
    "        #print(labels_list)\n",
    "        return bags_list, labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train==\"train\":\n",
    "            return len(self.train_labels_list)\n",
    "        elif self.train==\"val\":\n",
    "            return len(self.val_labels_list)\n",
    "        else:\n",
    "            return len(self.test_labels_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train==\"train\":\n",
    "            bag = self.train_bags_list[index]\n",
    "            label = [max(self.train_labels_list[index]), self.train_labels_list[index]]\n",
    "        elif self.train==\"val\":\n",
    "            bag = self.val_bags_list[index]\n",
    "            label = [max(self.val_labels_list[index]), self.val_labels_list[index]]\n",
    "        else:\n",
    "            bag = self.test_bags_list[index]\n",
    "            label = [max(self.test_labels_list[index]), self.test_labels_list[index]]\n",
    "\n",
    "        return bag, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "train_loader = data_utils.DataLoader(Stammering(train=\"train\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)\n",
    "test_loader = data_utils.DataLoader(Stammering(train=\"test\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)\n",
    "val_loader = data_utils.DataLoader(Stammering(train=\"val\"),batch_size=1,worker_init_fn=seed_worker,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.L = 128\n",
    "        self.D = 64\n",
    "        self.K = 1\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            nn.Linear(24,200),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(1, 20, kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(199,64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(1280, affine=False),\n",
    "            nn.Linear(1280, self.L),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L*self.K, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        \n",
    "        #H = H.view(-1, 256*5*4)\n",
    "        H = H.view(H.size(0), -1)\n",
    "        #print(H.shape)\n",
    "        H = self.feature_extractor_part2(H)  # NxL\n",
    "\n",
    "        A = self.attention(H)  # NxK\n",
    "\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "\n",
    "        A = F.softmax(A, dim=1)  # softmax over N\n",
    "\n",
    "        M = torch.mm(A, H)  # KxL\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.ge(Y_prob, 0.5).float()\n",
    "\n",
    "        return Y_prob, Y_hat, A\n",
    "\n",
    "\n",
    "\n",
    "    # AUXILIARY METHODS\n",
    "    def calculate_classification_error(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        _, Y_hat, _ = self.forward(X)\n",
    "        error = 1. - Y_hat.eq(Y).cpu().float().mean().item()\n",
    "\n",
    "        return error, Y_hat\n",
    "\n",
    "    def calculate_objective(self, X, Y):\n",
    "        Y = Y.float()\n",
    "        Y_prob, _, A = self.forward(X)\n",
    "        Y_prob = torch.clamp(Y_prob, min=1e-5, max=1. - 1e-5)\n",
    "        neg_log_likelihood = -1. * (Y * torch.log(Y_prob) + (1. - Y) * torch.log(1. - Y_prob))  # negative log bernoulli\n",
    "\n",
    "        return neg_log_likelihood, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model.cuda(),(1,24))\n",
    "# torch.save(model, \"modelarchitect_attn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH  0\n",
      "Train : Loss: 0.6846, Train error: 0.4322, Train acc : 0.5678294573643411\n",
      "Val : Loss: 0.6689, val error: 0.3898, Val acc :0.6101694915254238\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.6101694915254238\n",
      "EPOCH  1\n",
      "Train : Loss: 0.6784, Train error: 0.4360, Train acc : 0.563953488372093\n",
      "Val : Loss: 0.6816, val error: 0.4068, Val acc :0.5932203389830508\n",
      "Best validation accuracy  0.6101694915254238\n",
      "EPOCH  2\n",
      "Train : Loss: 0.6536, Train error: 0.3992, Train acc : 0.6007751937984496\n",
      "Val : Loss: 0.5808, val error: 0.3644, Val acc :0.635593220338983\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.635593220338983\n",
      "EPOCH  3\n",
      "Train : Loss: 0.6256, Train error: 0.3643, Train acc : 0.6356589147286822\n",
      "Val : Loss: 0.7372, val error: 0.4492, Val acc :0.5508474576271186\n",
      "Best validation accuracy  0.635593220338983\n",
      "EPOCH  4\n",
      "Train : Loss: 0.5646, Train error: 0.3023, Train acc : 0.6976744186046512\n",
      "Val : Loss: 0.6893, val error: 0.3898, Val acc :0.6101694915254238\n",
      "Best validation accuracy  0.635593220338983\n",
      "EPOCH  5\n",
      "Train : Loss: 0.5096, Train error: 0.2539, Train acc : 0.7461240310077519\n",
      "Val : Loss: 0.7890, val error: 0.4576, Val acc :0.5423728813559322\n",
      "Best validation accuracy  0.635593220338983\n",
      "EPOCH  6\n",
      "Train : Loss: 0.4773, Train error: 0.2364, Train acc : 0.7635658914728682\n",
      "Val : Loss: 0.8589, val error: 0.4322, Val acc :0.5677966101694916\n",
      "Best validation accuracy  0.635593220338983\n",
      "EPOCH  7\n",
      "Train : Loss: 0.4266, Train error: 0.2074, Train acc : 0.7926356589147286\n",
      "Val : Loss: 0.7965, val error: 0.3983, Val acc :0.6016949152542372\n",
      "Best validation accuracy  0.635593220338983\n",
      "EPOCH  8\n",
      "Train : Loss: 0.4005, Train error: 0.1957, Train acc : 0.8042635658914729\n",
      "Val : Loss: 0.9737, val error: 0.4153, Val acc :0.5847457627118644\n",
      "Best validation accuracy  0.635593220338983\n",
      "EPOCH  9\n",
      "Train : Loss: 0.3914, Train error: 0.1880, Train acc : 0.812015503875969\n",
      "Val : Loss: 1.4866, val error: 0.3559, Val acc :0.6440677966101694\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.6440677966101694\n",
      "EPOCH  10\n",
      "Train : Loss: 0.3402, Train error: 0.1550, Train acc : 0.8449612403100775\n",
      "Val : Loss: 1.1777, val error: 0.3729, Val acc :0.6271186440677966\n",
      "Best validation accuracy  0.6440677966101694\n",
      "EPOCH  11\n",
      "Train : Loss: 0.2996, Train error: 0.1182, Train acc : 0.8817829457364341\n",
      "Val : Loss: 1.5678, val error: 0.3644, Val acc :0.635593220338983\n",
      "Best validation accuracy  0.6440677966101694\n",
      "EPOCH  12\n",
      "Train : Loss: 0.2777, Train error: 0.1202, Train acc : 0.8798449612403101\n",
      "Val : Loss: 1.8872, val error: 0.3729, Val acc :0.6271186440677966\n",
      "Best validation accuracy  0.6440677966101694\n",
      "EPOCH  13\n",
      "Train : Loss: 0.2355, Train error: 0.0950, Train acc : 0.9050387596899225\n",
      "Val : Loss: 1.4917, val error: 0.3559, Val acc :0.6440677966101694\n",
      "---------State saved---------\n",
      "Best validation accuracy  0.6440677966101694\n",
      "EPOCH  14\n",
      "Train : Loss: 0.3175, Train error: 0.1337, Train acc : 0.8662790697674418\n",
      "Val : Loss: 1.4883, val error: 0.4322, Val acc :0.5677966101694916\n",
      "Best validation accuracy  0.6440677966101694\n"
     ]
    }
   ],
   "source": [
    "best_model = \"./saved_models/best_model_scripted_Attention_MIL_raw\"\n",
    "train_acc = []\n",
    "modelname=[]\n",
    "val_acc = []\n",
    "best_acc = 0\n",
    "for epoch in range(0, 15): #15 epochs seed =2021,11\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    train_error = 0.\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        bag_label = label[0]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = model.calculate_objective(data.float(), bag_label.float())\n",
    "        train_loss += loss.data[0]\n",
    "        error,y_pred = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        train_error += error\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "        ypred.extend(y_pred.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    trainacc = accuracy_score(y,ypred)\n",
    "    train_loss /= len(train_loader)\n",
    "    train_error /= len(train_loader)\n",
    "    print(\"EPOCH \",epoch)\n",
    "    print('Train : Loss: {:.4f}, Train error: {:.4f}, Train acc : {}'.format(train_loss.cpu().numpy()[0], \n",
    "                                                                                train_error,trainacc))\n",
    "    y =[]\n",
    "    ypred = []\n",
    "    val_error=0.\n",
    "    val_loss= 0.\n",
    "    model.eval()\n",
    "    for batch_idx, (data, label) in enumerate(val_loader):\n",
    "        bag_label = label[0]\n",
    "        data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        data, bag_label = Variable(data), Variable(bag_label)\n",
    "        loss, _ = model.calculate_objective(data.float(), bag_label.float())\n",
    "        val_loss += loss.data[0]\n",
    "        error, y_pred = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "        val_error += error\n",
    "        y_pred=y_pred.squeeze(1)\n",
    "\n",
    "        ypred.extend(y_pred.tolist())\n",
    "        y.extend(bag_label.tolist())\n",
    "    valacc = accuracy_score(y,ypred)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_error /= len(val_loader)\n",
    "    print('Val : Loss: {:.4f}, val error: {:.4f}, Val acc :{}'.format(val_loss.cpu().numpy()[0], val_error,valacc))\n",
    "    if valacc>=best_acc:\n",
    "        print(\"---------State saved---------\")\n",
    "        best_acc = valacc\n",
    "        best_state=model.state_dict()\n",
    "        torch.save(best_state, best_model+'_epoch_'+str(epoch)+\".pth\")\n",
    "        modelname.append(best_model+'_epoch_'+str(epoch)+\".pth\")\n",
    "    print('Best validation accuracy ',best_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9322033898305084 0.9298245614035087 0.9464285714285714 0.9137931034482759 0.9318965517241379 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "device = torch.device(\"cuda\")\n",
    "model=Attention()\n",
    "# best_state=torch.load(\"../best-models/Attention_MIL_best(stress)_F1_91.pth\")\n",
    "best_state=torch.load(modelname[-1])\n",
    "model.load_state_dict(best_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "test_loss = 0.\n",
    "correct = 0\n",
    "total = 0\n",
    "y =[]\n",
    "ypred = []\n",
    "test_error = 0.\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    bag_label = label[0]\n",
    "    instance_labels = label[1]\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    data, bag_label = Variable(data), Variable(bag_label)\n",
    "    loss, attention_weights = model.calculate_objective(data.float(), bag_label.float())\n",
    "    test_loss += loss.data[0]\n",
    "    error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "    test_error += error\n",
    "    predicted_label=predicted_label.squeeze(1)\n",
    "    ypred.extend(predicted_label.tolist())\n",
    "    y.extend(bag_label.tolist())\n",
    "    \n",
    "acc=accuracy_score(y,ypred)\n",
    "# print(y,ypred)\n",
    "tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "f1score=f1_score(y,ypred)\n",
    "precision=precision_score(y,ypred)\n",
    "recall=recall_score(y,ypred)\n",
    "roc=roc_auc_score(y,ypred)\n",
    "specificity=tn/(tn+fp)\n",
    "print(acc,f1score,precision,recall,roc,specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9322033898305084 0.9310344827586207 0.9310344827586207 0.9310344827586207 0.932183908045977 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model=Attention()\n",
    "# best_state=torch.load(\"../best-models/Attention_MIL_best(stress)_F1_91.pth\")\n",
    "best_state=torch.load(modelname[-1])\n",
    "model.load_state_dict(best_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# xTest=np.load(\"../extracted_features/0/win-1/GRP-3/input_features_val_grp3.npy\")\n",
    "# yTest=np.load(\"../extracted_features/0/win-1/GRP-3/labels_val_grp3.npy\")\n",
    "# testdataset = Dataset(xTest,yTest)\n",
    "testdataset = load_data('../Participant_wise_win-1','test')\n",
    "test_loader = data_utils.DataLoader(Stammering(train=\"test\"),worker_init_fn=seed_worker,batch_size=1,shuffle=True)\n",
    "attn_weights=[]\n",
    "test_loss = 0.\n",
    "correct = 0\n",
    "total = 0\n",
    "y =[]\n",
    "ypred = []\n",
    "test_error = 0.\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    bag_label = label[0]\n",
    "    instance_labels = label[1]\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    data, bag_label = Variable(data), Variable(bag_label)\n",
    "    loss, attention_weights = model.calculate_objective(data.float(), bag_label.float())\n",
    "    attn_weights.append(np.argmax(attention_weights.cpu().detach().numpy() ))\n",
    "    test_loss += loss.data[0]\n",
    "    error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "    test_error += error\n",
    "    predicted_label=predicted_label.squeeze(1)\n",
    "    ypred.extend(predicted_label.tolist())\n",
    "    y.extend(bag_label.tolist())\n",
    "acc=accuracy_score(y,ypred)\n",
    "tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "f1score=f1_score(y,ypred)\n",
    "precision=precision_score(y,ypred)\n",
    "recall=recall_score(y,ypred)\n",
    "roc=roc_auc_score(y,ypred)\n",
    "specificity=tn/(tn+fp)\n",
    "# print(best_model)\n",
    "print(acc,f1score,precision,recall,roc,specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9322033898305084 0.9285714285714286 0.9629629629629629 0.896551724137931 0.9316091954022988 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model=Attention()\n",
    "# best_state=torch.load(\"../best-models/Attention_MIL_best(stress)_F1_91.pth\")\n",
    "best_state=torch.load(modelname[-1])\n",
    "model.load_state_dict(best_state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# xTest=np.load(\"../extracted_features/0/win-2/GRP-3/input_features_val_grp3.npy\")\n",
    "# yTest=np.load(\"../extracted_features/0/win-2/GRP-3/labels_val_grp3.npy\")\n",
    "# testdataset = Dataset(xTest,yTest)\n",
    "testdataset = load_data('../Participant_wise_win-2','test')\n",
    "test_loader = data_utils.DataLoader(Stammering(train=\"test\"),worker_init_fn=seed_worker,batch_size=1,shuffle=True)\n",
    "attn_weights=[]\n",
    "test_loss = 0.\n",
    "correct = 0\n",
    "total = 0\n",
    "y =[]\n",
    "ypred = []\n",
    "test_error = 0.\n",
    "for batch_idx, (data, label) in enumerate(test_loader):\n",
    "    bag_label = label[0]\n",
    "    instance_labels = label[1]\n",
    "    data, bag_label = data.cuda(), bag_label.cuda()\n",
    "    data, bag_label = Variable(data), Variable(bag_label)\n",
    "    loss, attention_weights = model.calculate_objective(data.float(), bag_label.float())\n",
    "    attn_weights.append(np.argmax(attention_weights.cpu().detach().numpy() ))\n",
    "    test_loss += loss.data[0]\n",
    "    error, predicted_label = model.calculate_classification_error(data.float(), bag_label.float())\n",
    "    test_error += error\n",
    "    predicted_label=predicted_label.squeeze(1)\n",
    "    ypred.extend(predicted_label.tolist())\n",
    "    y.extend(bag_label.tolist())\n",
    "acc=accuracy_score(y,ypred)\n",
    "tn, fp, fn, tp = confusion_matrix(y,ypred).ravel()\n",
    "f1score=f1_score(y,ypred)\n",
    "precision=precision_score(y,ypred)\n",
    "recall=recall_score(y,ypred)\n",
    "roc=roc_auc_score(y,ypred)\n",
    "specificity=tn/(tn+fp)\n",
    "# print(best_model)\n",
    "print(acc,f1score,precision,recall,roc,specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0] [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(y,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (physio)",
   "language": "python",
   "name": "physio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
